1. ESPN Cricinfo has ODI batting stats for each batsman. The result is paginated across multiple pages. Count the number of ducks in page number 6.

Understanding the Data Source: ESPN Cricinfo's ODI batting statistics are spread across multiple pages, each containing a table of player data. Go to page number 6.
Setting Up Google Sheets: Utilize Google Sheets' IMPORTHTML function to import table data from the URL for page number 6.
Data Extraction and Analysis: Pull the relevant table from the assigned page into Google Sheets. Locate the column that represents the number of ducks for each player. (It is titled "0".) Sum the values in the "0" column to determine the total number of ducks on that page.
Impact
By automating the extraction and analysis of cricket batting statistics, CricketPro Insights can:

Enhance Analytical Efficiency: Reduce the time and effort required to manually gather and process player performance data.
Provide Timely Insights: Deliver up-to-date statistical analyses that aid teams and coaches in making informed decisions.
Scalability: Easily handle large volumes of data across multiple pages, ensuring comprehensive coverage of player performances.
Data-Driven Strategies: Enable the development of data-driven strategies for player selection, training focus areas, and game planning.
Client Satisfaction: Improve service offerings by providing accurate and insightful analytics that meet the specific needs of clients in the cricketing world.
What is the total number of ducks across players on page number 6 of ESPN Cricinfo's ODI batting stats?



2. Source: Utilize IMDb's advanced web search at https://www.imdb.com/search/title/ to access movie data.
Filter: Filter all titles with a rating between 2 and 5.
Format: For up to the first 25 titles, extract the necessary details: ID, title, year, and rating. The ID of the movie is the part of the URL after tt in the href attribute. For example, tt10078772. Organize the data into a JSON structure as follows:

[
  { "id": "tt1234567", "title": "Movie 1", "year": "2021", "rating": "5.8" },
  { "id": "tt7654321", "title": "Movie 2", "year": "2019", "rating": "6.2" },
  // ... more titles
]
Submit: Submit the JSON data in the text box below.
Impact
By completing this assignment, you'll simulate a key component of a streaming service's content acquisition strategy. Your work will enable StreamFlix to make informed decisions about which titles to license, ensuring that their catalog remains both diverse and aligned with subscriber preferences. This, in turn, contributes to improved customer satisfaction and retention, driving the company's growth and success in a competitive market.

What is the JSON data?

3. Write a web application that exposes an API with a single query parameter: ?country=. It should fetch the Wikipedia page of the country, extracts all headings (H1 to H6), and create a Markdown outline for the country. The outline should look like this:


## Contents

# Vanuatu

## Etymology

## History

### Prehistory

...
API Development: Choose any web framework (e.g., FastAPI) to develop the web application. Create an API endpoint (e.g., /api/outline) that accepts a country query parameter.
Fetching Wikipedia Content: Find out the Wikipedia URL of the country and fetch the page's HTML.
Extracting Headings: Use an HTML parsing library (e.g., BeautifulSoup, lxml) to parse the fetched Wikipedia page. Extract all headings (H1 to H6) from the page, maintaining order.
Generating Markdown Outline: Convert the extracted headings into a Markdown-formatted outline. Headings should begin with #.
Enabling CORS: Configure the web application to include appropriate CORS headers, allowing GET requests from any origin.
What is the URL of your API endpoint?


4. As part of this initiative, you are tasked with developing a system that automates the following:

API Integration and Data Retrieval: Use the BBC Weather API to fetch the weather forecast for Islamabad. Send a GET request to the locator service to obtain the city's locationId. Include necessary query parameters such as API key, locale, filters, and search term (city).
Weather Data Extraction: Retrieve the weather forecast data using the obtained locationId. Send a GET request to the weather broker API endpoint with the locationId.
Data Transformation: Extract the localDate and enhancedWeatherDescription from each day's forecast. Iterate through the forecasts array in the API response and map each localDate to its corresponding enhancedWeatherDescription. Create a JSON object where each key is the localDate and the value is the enhancedWeatherDescription.
The output would look like this:

{
  "2025-01-01": "Sunny with scattered clouds",
  "2025-01-02": "Partly cloudy with a chance of rain",
  "2025-01-03": "Overcast skies",
  // ... additional days
}
What is the JSON weather forecast description for Islamabad?


5. What is the maximum latitude of the bounding box of the city Lima in the country Peru on the Nominatim API?

API Integration: Use the Nominatim API to fetch geospatial data for a specified city within a country via a GET request to the Nominatim API with parameters for the city and country. Ensure adherence to Nominatim’s usage policies, including rate limiting and proper attribution.
Data Retrieval and Filtering: Parse the JSON response from the API. If multiple results are returned (e.g., multiple cities named “Springfield” in different states), filter the results based on the provided osm_id ending to select the correct city instance.
Parameter Extraction: Access the boundingbox attribute. Depending on whether you're looking for the minimum or maximum latitude, extract the corresponding latitude value.
Impact
By automating the extraction and processing of bounding box data, UrbanRide can:

Optimize Routing: Enhance route planning algorithms with precise geographical boundaries, reducing delivery times and operational costs.
Improve Fleet Allocation: Allocate vehicles more effectively across defined service zones based on accurate city extents.
Enhance Market Analysis: Gain deeper insights into regional performance, enabling targeted marketing and service improvements.
Scale Operations: Seamlessly integrate new cities into their service network with minimal manual intervention, ensuring consistent data quality.
What is the maximum latitude of the bounding box of the city Lima in the country Peru on the Nominatim API? Value of the maximum latitude


6. Search using the Hacker News RSS API for the latest Hacker News post mentioning Go and having a minimum of 85 points. What is the link that it points to?

Automate Data Retrieval: Utilize the HNRSS API to fetch the latest Hacker News posts. Use the URL relevant to fetching the latest posts, searching for topics and filtering by a minimum number of points.
Extract and Present Data: Extract the most recent <item> from this result. Get the <link> tag inside it.
Share the result: Type in just the URL in the answer.
What is the link to the latest Hacker News post mentioning Go having at least 85 points?



7. Using the GitHub API, find all users located in the city Berlin with over 140 followers.

When was the newest user's GitHub profile created?

API Integration and Data Retrieval: Leverage GitHub’s search endpoints to query users by location and filter them by follower count.
Data Processing: From the returned list of GitHub users, isolate those profiles that meet the specified criteria.
Sort and Format: Identify the "newest" user by comparing the created_at dates provided in the user profile data. Format the account creation date in the ISO 8601 standard (e.g., "2024-01-01T00:00:00Z").
Impact
By automating this data retrieval and filtering process, CodeConnect gains several strategic advantages:

Targeted Recruitment: Quickly identify new, promising talent in key regions, allowing for more focused and timely recruitment campaigns.
Competitive Intelligence: Stay updated on emerging trends within local developer communities and adjust talent acquisition strategies accordingly.
Efficiency: Automating repetitive data collection tasks frees up time for recruiters to focus on engagement and relationship-building.
Data-Driven Decisions: Leverage standardized and reliable data to support strategic business decisions in recruitment and market research.
Enter the date (ISO 8601, e.g. "2024-01-01T00:00:00Z") when the newest user joined GitHub.


8. Create a scheduled GitHub action that runs daily and adds a commit to your repository. The workflow should:

Use schedule with cron syntax to run once per day (must use specific hours/minutes, not wildcards)
Include a step with your email 23f2004644@ds.study.iitm.ac.in in its name
Create a commit in each run
Be located in .github/workflows/ directory
After creating the workflow:

Trigger the workflow and wait for it to complete
Ensure it appears as the most recent action in your repository
Verify that it creates a commit during or within 5 minutes of the workflow run
Enter your repository URL (format: https://github.com/USER/REPO):


9. This file,  contains a table of student marks in Maths, Physics, English, Economics, and Biology.

Calculate the total Maths marks of students who scored 37 or more marks in Economics in groups 19-51 (including both groups).

Data Extraction:: Retrieve the PDF file containing the student marks table and use PDF parsing libraries (e.g., Tabula, Camelot, or PyPDF2) to accurately extract the table data into a workable format (e.g., CSV, Excel, or a DataFrame).
Data Cleaning and Preparation: Convert marks to numerical data types to facilitate accurate calculations.
Data Filtering: Identify students who have scored marks between 37 and Economics in groups 19-51 (including both groups).
Calculation: Sum the marks of the filtered students to obtain the total marks for this specific cohort.
By automating the extraction and analysis of student marks, EduAnalytics empowers Greenwood High School to make informed decisions swiftly. This capability enables the school to:

Identify Performance Trends: Quickly spot areas where students excel or need additional support.
Allocate Resources Effectively: Direct teaching resources and interventions to groups and subjects that require attention.
Enhance Reporting Efficiency: Reduce the time and effort spent on manual data processing, allowing educators to focus more on teaching and student engagement.
Support Data-Driven Strategies: Use accurate and timely data to shape educational strategies and improve overall student outcomes.
What is the total Maths marks of students who scored 37 or more marks in Economics in groups 19-51 (including both groups)?


10. As part of the Documentation Transformation Project, you are a junior developer at EduDocs tasked with developing a streamlined workflow for converting PDF files to Markdown and ensuring their consistent formatting. This project is critical for supporting EduDocs' commitment to delivering high-quality, accessible educational resources to its clients.

 has the contents of a sample document.

Convert the PDF to Markdown: Extract the content from the PDF file. Accurately convert the extracted content into Markdown format, preserving the structure and formatting as much as possible.
Format the Markdown: Use Prettier version 3.4.2 to format the converted Markdown file.
Submit the Formatted Markdown: Provide the final, formatted Markdown file as your submission.
Impact
By completing this exercise, you will contribute to EduDocs Inc.'s mission of providing high-quality, accessible educational resources. Automating the PDF to Markdown conversion and ensuring consistent formatting:

Enhances Productivity: Reduces the time and effort required to prepare documentation for clients.
Improves Quality: Ensures all documents adhere to standardized formatting, enhancing readability and professionalism.
Supports Scalability: Enables EduDocs to handle larger volumes of documentation without compromising on quality.
Facilitates Integration: Makes it easier to integrate Markdown-formatted documents into various digital platforms and content management systems.
What is the markdown content of the PDF, formatted with prettier@3.4.2?